random_seed: 42
output_path: "experiments/results" # Path to save the model
experiment_name: "baseline"
save_model: False # Whether to save the model [True, False]
inference: True # Whether to perform inference [True, False]
demo: True # Whether to run the train function for demo [True, False]

data:
  name: "probe_images"
  root_dir: "datasets/probe_images_small"
  batch_size: 24
  num_workers: 6


model:
  model: "vgg16" # Name of the model
  tag: 'flyability' # Model's tag for wandb logging
  num_classes: 2 # Number of classes in the dataset (needed for fast RCNN model)
  load_model_path: "experiments/results/vgg16/20250109-223606_8c5e7/20250109-223606_8c5e7.pth" # Path to load the model


optimizer:
  lr: 0.001
  weight_decay: 0
  optimizer: "adam" # Name of the optimizer ["adam", "sgd"]
  decrease_every: 150 # Frequency of the learning rate decrease
  lr_divisor: 2 # Rate of the learning rate decrease

training:
  epochs: 20 # Number of epochs for training
  validate_per_epoch: 2 # Periodicity to evaluate the model
  loss: "mse" # Name of the loss function, which may correpond to the model as they have custom loss functions ["vgg16", "fasterrcnn_resnet50_fpn", "mse", "iou"]
  apply_augmentation: False # Whether to apply data augmentation [True, False]

logging:
  project: "Flybility" # Name of the wandb project
  mode: "disabled" # Whether to log to wand [online, offline, disabled]


distributed:
  use_distributed: False # Whether to use distributed training [True, False]
  backend: "nccl" # Backend for distributed training ["nccl", "gloo"]
  local_rank: 0 # Local rank for distributed training
  world_size: 1 # Number of processes for distributed training
  